{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These libraries will be used for our URL_classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_calculation(labels, predicition):\n",
    "    print(prediction)\n",
    "    print(\"Accuracy score: {}\".format(metrics.accuracy_score(labels, prediction)))\n",
    "    print(\"Precision_score: {}\".format(metrics.precision_score(labels, prediction, average='micro')))\n",
    "    print(\"Recall_score: {}\".format(metrics.recall_score(labels, prediction, average='micro')))\n",
    "    print(\"F1 score: {}\".format(metrics.f1_score(labels, prediction, average='micro')))\n",
    "    print('Confusion matrix: \\n{}'.format(metrics.confusion_matrix(labels, prediction)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A function responsible for accuracy, precision, recall, f1 score calculation. Also it provides Confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = 'URL-categorization-DFE.csv'\n",
    "limiter = 2000\n",
    "top = 50\n",
    "cv_number = 5\n",
    "reader = csv.reader(open(file), delimiter=',')\n",
    "header = next(reader)\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "language_whitelist = ['en']\n",
    "domains_whitelist = ['com', 'org', 'net', '.us', '.uk', '.au', '.ca']\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Determine file path with all URL_classification data, set how many lines we want to read(limiter);\n",
    "top - a number which represents how many most frequent words is stored for each category.\n",
    "char_blacklist, stopwords, language_whitelist, domains_whitelist, english_vocab - these variables are for URL filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in reader:\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Store data to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL parsing and filtering\n",
      "----\n",
      "URL: 000webhost.com\n",
      "Lang: en\n",
      "CATEGORY: Internet_and_Telecom\n",
      "0\n",
      "Nr:  1\n",
      "----\n",
      "URL: 0calc.com\n",
      "Lang: en\n",
      "CATEGORY: Science\n",
      "10\n",
      "Nr:  2\n",
      "----\n",
      "URL: 10bet.com\n",
      "Lang: en\n",
      "CATEGORY: Gambling\n",
      "26\n",
      "Nr:  3\n"
     ]
    }
   ],
   "source": [
    "tokens_list = []\n",
    "filter_data = []\n",
    "counter = 0\n",
    "print('URL parsing and filtering')\n",
    "for url_counter, row in enumerate(data):\n",
    "    if url_counter >= limiter:\n",
    "        break\n",
    "    if row[5] != 'Not_working' and float(row[6]) > 0.5:\n",
    "        try:\n",
    "            url = 'http://' + row[-1]\n",
    "            html = urlopen(url, timeout=1).read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "            text_vocab = set(w.lower() for w in text if w.lower().isalpha())\n",
    "            if detect(text) not in language_whitelist or (row[-1][-3:] not in domains_whitelist and row[-1][-3:] not in domains_whitelist):\n",
    "                continue\n",
    "            counter += 1\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tokens_list += [nltk.word_tokenize(text)]\n",
    "            print('----')\n",
    "            print('URL: ' + row[-1])\n",
    "            print('Lang: '+ detect(text))\n",
    "            print('CATEGORY: ' + row[5])\n",
    "            print(url_counter)\n",
    "            print('Nr: ', counter)\n",
    "            filter_data += [row[5]]\n",
    "\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Filter URL for analyzing most frequent words and excluding stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Filtering categories')\n",
    "f1 = nltk.FreqDist(filter_data).most_common()\n",
    "f2 = list(category for category, number in f1 if number >= cv_number)\n",
    "all_categories = list(set(f2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Determine which categories are suitable for machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('CREATING LABELS DATA.')\n",
    "labels = []\n",
    "counter = 0\n",
    "for index, word in enumerate(filter_data):\n",
    "    if word in all_categories:\n",
    "        labels += [all_categories.index(word)]\n",
    "    else:\n",
    "        tokens_list.pop(index - counter)\n",
    "        counter += 1\n",
    "save = labels\n",
    "labels = np.array(labels).reshape((len(labels), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating labels for machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('CREATING FREQUENT WORDS LIST..')\n",
    "freq_words = []\n",
    "for tokens in tokens_list:\n",
    "    allWordDist = nltk.FreqDist(w.lower() for w in tokens)\n",
    "    allWordExceptStopDist = nltk.FreqDist(\n",
    "        w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "    all_words = [i for i in allWordExceptStopDist]\n",
    "    mostCommon = allWordExceptStopDist.most_common(top)\n",
    "    freq_words += [word for word, number in mostCommon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating a list of most frequent words for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('CREATING FEATURES DATA...')\n",
    "features = np.zeros(pow(len(tokens_list), 2) * top).reshape(len(tokens_list), len(tokens_list) * top)\n",
    "for index, line in enumerate(tokens_list):\n",
    "    for word in line:\n",
    "        if word in freq_words:\n",
    "            features[index][freq_words.index(word)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating features list for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, r = labels.shape\n",
    "labels = labels.reshape(c,)\n",
    "lr = LogisticRegression()\n",
    "prediction = cross_val_predict(lr, features, labels, cv=cv_number)\n",
    "score_calculation(labels, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict values and calculating score by using Logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
