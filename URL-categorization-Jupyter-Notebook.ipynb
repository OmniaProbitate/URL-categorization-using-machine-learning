{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "These libraries will be used for our URL_classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use this command if you have any errors on importing nltk library. It will open a nltk meniu with download and update options. If it's still missing some libraries, it needs to install manually by writing nltk.download('library name') where library name is missing library name which asserts error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for accuracy calculations\n",
    "A function responsible for accuracy, precision, recall, f1 score calculation. Also it provides Confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_calculation(labels, prediction):\n",
    "    y_true = pd.Series(labels)\n",
    "    y_pred = pd.Series(prediction)\n",
    "    print(str(lr))\n",
    "    print('Confusion matrix: \\n{}'.format(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)))\n",
    "    print(classification_report(labels, prediction))\n",
    "    print(\"Accuracy score: {}\".format(metrics.accuracy_score(labels, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup\n",
    "Determine file path with all URL_classification data, set how many lines we want to read(limiter);\n",
    "top - a number which represents how many most frequent words is stored for each category.\n",
    "char_blacklist, stopwords, language_whitelist, domains_whitelist, english_vocab - these variables are for URL filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:46:48.878882\n"
     ]
    }
   ],
   "source": [
    "file = 'URL-categorization-DFE.csv'\n",
    "limiter = 500 #Number of URL for analyzing. 2000 ~ 40 min, 1000-16 min. MINIMUM VALUE=150; MAXIMUM VALUE = 30000\n",
    "cv_number = 5 #cv - Cross valid classification parameter:\n",
    "'''\n",
    "MINIMUL REQUIRMENTS\n",
    "if cv_number = 2; limiter >= 150; ~2 min\n",
    "if cv_number = 3; limiter >= 250; ~4 min\n",
    "if cv_number = 4; limiter >= 350; ~6 min\n",
    "if cv_number = 5; limiter >= 500; ~9 min\n",
    "'''\n",
    "top = 15\n",
    "reader = csv.reader(open(file), delimiter=',')\n",
    "header = next(reader)\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "language_whitelist = ['en']\n",
    "domains_whitelist = ['com', 'org', 'net', '.us', '.uk', '.au', '.ca']\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of cv_number and limiter \n",
    "Check if cv_number and limiter meets minimum requirements to prevent errors using cross valid clasification method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limiter = 500\n",
      "cv_number = 5\n"
     ]
    }
   ],
   "source": [
    "if cv_number < 2:\n",
    "    cv_number = 2\n",
    "if limiter <= 500:\n",
    "    if limiter >= 150 and cv_number > 2:\n",
    "        cv_number = 2\n",
    "    if limiter >= 250 and cv_number < 3 or cv_number > 3:\n",
    "        cv_number = 3\n",
    "    if limiter >= 350 and cv_number < 4 or cv_number > 4:\n",
    "        cv_number = 4\n",
    "    if limiter >= 500 and cv_number < 5 or cv_number > 5:\n",
    "        cv_number = 5\n",
    "    if limiter < 150:\n",
    "        limiter = 150\n",
    "        cv_number = 2\n",
    "print('limiter = {}'.format(limiter))\n",
    "print('cv_number = {}'.format(cv_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Store data to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in reader:\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Downloading and analyzing URL\n",
    "Downloading each URL content. Filter URL for analyzing most frequent words and excluding stop words. Take note that this process take a lot of time. For example: downloading 2000 first URL takes about 40 minutes. Limiter value is how many URL will be downloaded and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL parsing and filtering\n",
      "10 | 1 |URL: 0calc.com| CATEGORY: Science\n",
      "26 | 2 |URL: 10bet.com| CATEGORY: Gambling\n",
      "28 | 3 |URL: 1100ad.com| CATEGORY: Books_and_Literature\n",
      "43 | 4 |URL: 123vidz.com| CATEGORY: Arts_and_Entertainment\n"
     ]
    }
   ],
   "source": [
    "tokens_list = []\n",
    "filter_data = []\n",
    "counter = 0\n",
    "print('URL parsing and filtering')\n",
    "for url_counter, row in enumerate(data):\n",
    "    if url_counter >= limiter:\n",
    "        break\n",
    "    if row[5] != 'Not_working' and float(row[6]) > 0.5:\n",
    "        try:\n",
    "            url = 'http://' + row[-1]\n",
    "            html = urlopen(url, timeout=1).read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "            text_vocab = set(w.lower() for w in text if w.lower().isalpha())\n",
    "            if detect(text) not in language_whitelist or (row[-1][-3:] not in domains_whitelist and row[-1][-3:] not in domains_whitelist):\n",
    "                continue\n",
    "            counter += 1\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tokens_list += [nltk.word_tokenize(text)]\n",
    "            print('{} | {} |URL: {}| CATEGORY: {}'.format(url_counter, counter, row[-1], row[5]))\n",
    "            filter_data += [row[5]]\n",
    "\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Filtering categories\n",
    "Determine which categories are suitable for machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtering categories')\n",
    "f1 = nltk.FreqDist(filter_data).most_common()\n",
    "f2 = list(category for category, number in f1 if number >= cv_number)\n",
    "all_categories = list(set(f2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Labels\n",
    "Creating labels for machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CREATING LABELS DATA.')\n",
    "labels = []\n",
    "counter = 0\n",
    "for index, word in enumerate(filter_data):\n",
    "    if word in all_categories:\n",
    "        labels += [all_categories.index(word)]\n",
    "    else:\n",
    "        tokens_list.pop(index - counter)\n",
    "        counter += 1\n",
    "save = labels\n",
    "labels = np.array(labels).reshape((len(labels), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Most frequent words\n",
    "Creating a list of most frequent words for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CREATING FREQUENT WORDS LIST..')\n",
    "freq_words = []\n",
    "for tokens in tokens_list:\n",
    "    allWordDist = nltk.FreqDist(w.lower() for w in tokens)\n",
    "    allWordExceptStopDist = nltk.FreqDist(\n",
    "        w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "    all_words = [i for i in allWordExceptStopDist]\n",
    "    mostCommon = allWordExceptStopDist.most_common(top)\n",
    "    freq_words += [word for word, number in mostCommon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Features\n",
    "Creating features list for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CREATING FEATURES DATA...')\n",
    "features = np.zeros(pow(len(tokens_list), 2) * top).reshape(len(tokens_list), len(tokens_list) * top)\n",
    "for index, line in enumerate(tokens_list):\n",
    "    for word in line:\n",
    "        if word in freq_words:\n",
    "            features[index][freq_words.index(word)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Display all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number, word in enumerate(all_categories):\n",
    "    print(number, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and performance score\n",
    "Predict values and calculating score by using Logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, r = labels.shape\n",
    "labels = labels.reshape(c,)\n",
    "print('************ Logistic Regression ************')\n",
    "lr = LogisticRegression()\n",
    "prediction = cross_val_predict(lr, features, labels, cv=cv_number)\n",
    "score_calculation(labels, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict values and calculating score by using Decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('************ Decision Tree ************')\n",
    "lr = DecisionTreeClassifier()\n",
    "prediction = cross_val_predict(lr, features, labels, cv=cv_number)\n",
    "score_calculation(labels, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict values and calculating score by using KNeighbors classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('************ KNeighbors ************')\n",
    "lr = KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\")\n",
    "prediction = cross_val_predict(lr, features, labels, cv=cv_number)\n",
    "score_calculation(labels, prediction)\n",
    "\n",
    "print(datetime.datetime.now().time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
